---
title: "Shopee_Order_Brush"
author: "Kwok Cheong, Shu Min, Amy"
date: "4/10/2022"
output:
  html_document:
    fig_width: 14
    fig_height: 8
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bring in library

```{r echo=TRUE,results= 'hide',error=FALSE,message=FALSE,warning=FALSE}
library(foreign)
library(ggplot2)
library(dplyr)
library(e1071)
library(corrplot)
library(randomForest)
library(naivebayes)
library(caret)
library(vcd)
library(gridExtra)
library(ggeffects)
library(smotefamily)
library(ROCR)
library(mlbench)
```

## Data Preprocess

```{r error=FALSE,message=FALSE,warning=FALSE}
data <- read.csv("main_data_cleaned_v4.csv", header = TRUE)

# data preprocess
data$Orderbrush <- as.factor(data$Orderbrush)
data$Verified <- as.factor(data$Verified)
data$Shop.Followers.Cat <- as.factor(data$Shop.Followers.Cat)
data$Shop.Rating <- as.factor(data$Shop.Rating)

str(data)

numData <- data[,3:15]

set.seed(122)
train_index <- createDataPartition(numData$Orderbrush, p=0.75, list= FALSE)
train <- numData[train_index, ] # 75% of the data
test <- numData[-train_index, ] # 25% of the data


```
## Data preprocess 2

```{r error=FALSE,message=FALSE,warning=FALSE}

# for plotting purposes (corplot)
data_numeric <- subset(data, select = -c(Shop.Followers.Cat, Verified, Shop.Rating))
data_numeric <- data_numeric[,2:12]
data_numeric$Orderbrush <- as.numeric(data_numeric$Orderbrush) - 1

# if you only wan numeric and non categorical data, mainly using to do SMOTE
train_numeric <- subset(train, select = -c(Shop.Followers.Cat, Verified, Shop.Rating))
test_numeric <- subset(test, select = -c(Shop.Followers.Cat, Verified, Shop.Rating))
train_numeric$Orderbrush <- as.numeric(train_numeric$Orderbrush) - 1
test_numeric$Orderbrush <- as.numeric(test_numeric$Orderbrush) - 1

# 0 is non fraud, 1 is fraud
table(train_numeric$Orderbrush)

# perform smote on train data 
train_smote <- SMOTE(X = train_numeric, target=train_numeric$Orderbrush, dup_size = 10)
train_smote_data <- train_smote$data
train_smote_data$Orderbrush <- as.factor(train_smote_data$Orderbrush)
levels(train_smote_data$Orderbrush) <- c("No", "Yes")
train_smote_data <- subset(train_smote_data, select = -c(class))
table(train_smote_data$Orderbrush)
```

## General Visualization Plot 
```{r error=FALSE,message=FALSE,warning=FALSE}

ggplot(data, aes(x=Orderbrush, fill=Shop.Followers.Cat) ) + geom_bar(position="dodge", stat= "count") + ylim(0,1000)


ggplot(data, aes(x=Orderbrush, y=Shop.Rating.Value) ) + geom_boxplot(aes(fill=Shop.Rating.Value))
ggplot(data, aes(x=Orderbrush, y=Count.of.Order) ) + geom_boxplot(aes(fill=Count.of.Order)) + ylim(0,1000)
ggplot(data, aes(x=Orderbrush, y=Hour) ) + geom_boxplot(aes(fill=Hour))
ggplot(data, aes(x=Orderbrush, y=Total.Comments) ) + geom_boxplot(aes(fill=Total.Comments)) + ylim(0,40000)
ggplot(data, aes(x=Orderbrush, y=Bad.Comments) ) + geom_boxplot(aes(fill=Bad.Comments)) + ylim(0,2000)

plot(data$Total.Comments, col=c("lightgrey","red")[data$Orderbrush])
plot(data$Count.of.Order, col=c("lightgrey","red")[data$Orderbrush])


ggplot(data, aes(Shop.Rating.Value, Orderbrush, colour=Shop.Rating.Value)) + geom_point(alpha=0.3) + theme(legend.position = "top") + labs(title="template") + geom_smooth(method="loess", se=FALSE)

```

## Correlation plot
```{r error=FALSE,message=FALSE,warning=FALSE}

cor_data <- cor(x = data_numeric[sapply(data_numeric, is.numeric)], method="pearson")
round(cor_data,2)
correlation_plot <- corrplot(cor_data)
```

## Random Forest (Default - Train test)
```{r error=FALSE,message=FALSE,warning=FALSE}

train_numeric$Orderbrush <- as.factor(train_numeric$Orderbrush)
test_numeric$Orderbrush <- as.factor(test_numeric$Orderbrush)
levels(train_numeric$Orderbrush) <- c("No", "Yes")
levels(test_numeric$Orderbrush) <- c("No", "Yes")

rf_model_default <- randomForest(formula = Orderbrush ~ ., data=train_numeric)
p_rf <- predict(rf_model_default, newdata=test_numeric)
confusionMatrix(as.factor(test_numeric$Orderbrush), p_rf ,mode = "prec_recall", positive="Yes")

varImpPlot(rf_model_default)
varImp(rf_model_default)

```


## Random Forest (SMOTE)

```{r error=FALSE,message=FALSE,warning=FALSE}
rf_model <- randomForest(formula = Orderbrush ~ ., data=train_smote_data )
p_rf <- predict(rf_model, newdata=test_numeric)
confusionMatrix(test_numeric$Orderbrush, p_rf)

varImpPlot(rf_model)
varImp(rf_model)
```

## Random Forest (10 fold With undersample and preprocessing)

We use the caret package to run a repeated cross validation over 10 folds. 

```{r error=FALSE,message=FALSE,warning=FALSE}
# RF - CARET
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

random_f_model <- caret::train(Orderbrush ~ .,
                                   data = train,
                                   method = "rf",
                                   preProcess = c("scale", "center"),
                                   trControl = ctrl)

p_rf2 <- predict(random_f_model, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_rf2, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
varImp(random_f_model, scale=FALSE)
plot(varImp(random_f_model, scale=FALSE))   
```
## Random Forest (10 fold with undersample and scale center preprocessing) 

```{r error=FALSE,message=FALSE,warning=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling="down")

random_f_model <- caret::train(Orderbrush ~ .,
                                   data = train,
                                   method = "rf",
                                   preProcess = c("scale", "center"),
                                   trControl = ctrl)

p_rf2 <- predict(random_f_model, newdata=test)
confusionMatrix(as.factor(test_numeric$Orderbrush), p_rf2, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
varImp(random_f_model, scale=FALSE)
plot(varImp(random_f_model, scale=FALSE)) 

```

## Random Forest (10 fold with undersample and normalization preprocessing)

```{r error=FALSE,message=FALSE,warning=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling="down")

random_f_model <- caret::train(Orderbrush ~ Shop.Follower.Count + Shop.Rating.Value + Bad.Comments + Normal.Comments + Good.Comments + Total.Comments + Shop.Response.Rate + Count.of.Order + Hour,
                                   data = train,
                                   method = "rf",
                                   preProcess = c("range"),
                                   trControl = ctrl)

p_rf2 <- predict(random_f_model, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_rf2, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
varImp(random_f_model, scale=FALSE)
plot(varImp(random_f_model, scale=FALSE)) 

```


## Naive Bayes 

```{r error=FALSE,message=FALSE,warning=FALSE}

model2 <- naive_bayes(Orderbrush~., data= train, laplace=1);
p <- predict(model2, test)

confusionMatrix(test$Orderbrush, p, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)

model3 <- naive_bayes(Orderbrush~., data= train_smote_data, laplace=1)
p_smote <- predict(model3, test)
confusionMatrix(test$Orderbrush, p_smote, mode = "prec_recall", positive="Yes")


```


## Logistic Regression method 1 

Receiver Operator Characteristic (ROC) and Area Under the Curve (AUC) are typical performance measurements for a binary classifier. 

ROC curve closer to the top-left corner indicate a better performance.

AUC between 0.5 to 1 mean the model with good predictive ability.

```{r error=FALSE,message=FALSE,warning=FALSE}

train$Orderbrush <- as.factor(train$Orderbrush)
model3 <- glm(Orderbrush ~., family = binomial(link='logit'), data = train)
summary(model3)

fitted.results <- predict(model3, newdata = test, type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

misClasificError <- mean(fitted.results != test$Orderbrush)
print(paste('Accuracy',1-misClasificError))

p <- predict(model3, newdata = test, type="response")
pr <- prediction(p, test$Orderbrush)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)

```


## Logistic Regression method 2 

We used glm method which specifies that we will fit a generalized linear model.

```{r error=FALSE,message=FALSE,warning=FALSE}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

metric <- "Accuracy"
tuneLength <- 10

train$Orderbrush <- as.factor(train$Orderbrush)

logistic_r_model <- caret::train(Orderbrush ~ .,
                               data = train,
                               method = "glm",
                               metric=metric,
                               preProcess = c("scale", "center"),
                               trControl = ctrl,
                               tuneLength = tuneLength)

print(logistic_r_model)
p_lr <- predict(logistic_r_model, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_lr, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
plot(varImp(logistic_r_model, scale=FALSE))

```

## Support Vector Machines (SVM) Linear Classifier

We used SVM linear classifier, the variables are normalized to make their scale comparable.

Tuning parameter C (Cost) is to determine the possible misclassifications.
The higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.

We build using the default C = 1. The accuracy of this model is 0.8880263.

```{r error=FALSE,message=FALSE,warning=FALSE}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

train$Orderbrush <- as.factor(train$Orderbrush)

svm_model <- caret::train(Orderbrush ~ .,
                                 data = train,
                                 method = "svmLinear",
                                 preProcess = c("scale", "center"),
                                 trControl = ctrl)

print(svm_model)
p_svm <- predict(svm_model, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_svm, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
plot(varImp(svm_model, scale=FALSE))

# Make predictions on the test data
predicted.classes <- svm_model %>% predict(test)
# Compute model accuracy rate
accuracy_rate <- mean(predicted.classes == test$Orderbrush)
print(accuracy_rate)

```


## Support Vector Machines (SVM) Linear with Choice of Cost

We compute SVM for a grid values of C and choose automatically the final model for predictions.

The choice of C = 0.2105263 provides an Accuracy of 0.8817566 which is a slight improvement over the first SVM where C was held constant at 1 (Accuracy = 0.8880263).

```{r error=FALSE,message=FALSE,warning=FALSE}

svm_model2 <- caret::train(Orderbrush ~ .,
                          data = train,
                          method = "svmLinear",
                          preProcess = c("scale", "center"),
                          trControl = ctrl,
                          tuneGrid = expand.grid(C = seq(0, 2, length = 20)))

print(svm_model2)
p_svm2 <- predict(svm_model2, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_svm2, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
plot(varImp(svm_model2, scale=FALSE))

# Plot model accuracy vs different values of Cost
plot(svm_model2)

# Make predictions on the test data
predicted.classes2 <- svm_model2 %>% predict(test)
# Compute model accuracy rate
accuracy_rate2 <- mean(predicted.classes2 == test$Orderbrush)
print(accuracy_rate2)

```


## Support Vector Machines (SVM) Non-Linear Kernel (Radial)

We used radial kernel function to build non-linear SVM classifier.

```{r error=FALSE,message=FALSE,warning=FALSE}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

svm_model3 <- caret::train(Orderbrush ~ .,
                          data = train,
                          method = "svmRadial",
                          preProcess = c("scale", "center"),
                          trControl = ctrl,
                          tuneLength = 10)

print(svm_model3)
p_svm3 <- predict(svm_model3, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_svm3, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
plot(varImp(svm_model3, scale=FALSE))

# Make predictions on the test data
predicted.classes3 <- svm_model3 %>% predict(test)
# Compute model accuracy rate
accuracy_rate3 <- mean(predicted.classes3 == test$Orderbrush)
print(accuracy_rate3)

```


## Support Vector Machines (SVM) Non-Linear Kernel (Polynomial)

We used polynomial kernel function to build non-linear SVM classifier.

```{r error=FALSE,message=FALSE,warning=FALSE}

ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

svm_model4 <- caret::train(Orderbrush ~ .,
                          data = train,
                          method = "svmPoly",
                          preProcess = c("scale", "center"),
                          trControl = ctrl,
                          tuneLength = 4)

print(svm_model4)
p_svm4 <- predict(svm_model4, newdata=test)
confusionMatrix(as.factor(test$Orderbrush), p_svm4, mode = "prec_recall", positive="Yes")
table(test$Orderbrush)
plot(varImp(svm_model4, scale=FALSE))

# Make predictions on the test data
predicted.classes4 <- svm_model4 %>% predict(test)
# Compute model accuracy rate
accuracy_rate4 <- mean(predicted.classes4 == test$Orderbrush)
print(accuracy_rate4)

```


## Support Vector Machines (SVM) Comparison

Compare all 4 methods, linear and non-linear.

It shown that the SVM classifier using non-linear kernel Radial gives a better result compared to the linear model.

```{r error=FALSE,message=FALSE,warning=FALSE}

df <- tibble(Model = c('SVM Linear','SVM Linear w/ choice of cost','SVM Radial','SVM Poly'), Accuracy = c(accuracy_rate, accuracy_rate2, accuracy_rate3, accuracy_rate4))
df %>% arrange(Accuracy)

```